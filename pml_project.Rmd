---
title: "PML_Project"
author: "Dina ElMaamoun"
date: "Sunday, June 21, 2015"
output: html_document
---
##Introduction
  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The purpose of this analysis is to predict the quality of the exercise using the above collected data. The classes are "A","B", & "C", where "A" would be the correct way to perform the exercise.

## Data Loading
  
```{r, eval= FALSE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv")
```
```{r}
pml_training<- read.csv("pml-training.csv",header = TRUE, sep = ",")
pml_validation<- read.csv("pml-testing.csv",header = TRUE, sep = ",")
```

##Data Cleanup
  
The data is shown to have a lot of empty/NA columns that are useless to our analysis, and so should be removed.  In Addition, some columns like the timestamps are not relevant to the analysis and should be removed as well.
```{r}
library(caret)
pml_training[pml_training=="NA"]<- NA
pml_validation[pml_validation=="NA"]<- NA
pml_training[pml_training==""]<- NA
pml_validation[pml_validation==""]<- NA
pml_training$classe<- factor(pml_training$classe)
df_training<-pml_training[,colSums(is.na(pml_training)) <100]
df_validation<-pml_validation[,colSums(is.na(pml_validation)) <10]
df_training<-df_training[!(is.na(df_training$classe)),]
df_training<-subset(df_training, select = -c(2:6))
df_validation<-subset(df_validation, select = -c(2:6))
```

##Data Slicing & Cross Validation
   
In order to create the model, the data is split into a training set and a test set. the test set is used to cross-validate the training set and make sure it is correct.  The training data will be set to 60% while the test/cross validation data will be set to 40% of the training data.
  
```{r}
inTrain<- createDataPartition(y = df_training$classe, p = 0.6, list=FALSE)
training<- df_training[inTrain,]
testing<- df_training[-inTrain,]
```

## Model Creation & Prediction
  The model will be built using random forrest method since it has high accuracy.
```{r}
set.seed(12345)
model1<- train(classe ~ .,data=training,method = "rf",prox =TRUE, preProcess = "pca")
pred_train<- predict(model1, training)
table(pred_train,training$classe)
```

We can also see that the most important variables according to the model are as follows:
```{r}
varImp(model1)
```
  It is worth noting that the random forrest algorithm is time consuming and so this might not scale well.

## Sample Error
  
The out of sample error should be explained by the accuracy; which elements in the testing set were predicted wrong. According to our training set, if we check the actuals versus the predicted values we see the following:
```{r}
pred_train<- predict(model1, traiing)
table(pred_test,training$classe)
```
  
As for the test set, we see the following values:
```{r}
pred_test<- predict(model1, testing)
abc<- table(pred_test,testing$classe)
abc
```
Accordingly, the accuracy in this case for the test set: 
```{r}
num<- sum(abc[1,2:5],abc[2,c(1,3:5)],abc[3,c(1:2,4:5)],abc[4,c(1:3,5)],abc[5,1:4])
den<- sum(abc)
error<- num/den*100
error
